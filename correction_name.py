# -*- coding: utf-8 -*-
"""Correction Name.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l5GF2gaAfK-EjSOSN05REMAJMvdoitOi
"""

# versi 2 yg JAGOAN
# Train dataset kecil MANTAP

import tensorflow as tf
import pandas as pd
import numpy as np
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if (logs.get('val_accuracy')>0.8353):
          self.model.stop_training = True

def solution_B4():
  # Load dataset yg super duper banyak
  df_benar = pd.read_csv('https://raw.githubusercontent.com/Zahraa02/Roadgo/main/nama_penjual%20(single).csv')['name'] # cb pake lgsg docs yg diupload # cb pake dataset yg kecil
  df_salah = pd.read_csv('https://raw.githubusercontent.com/Zahraa02/Roadgo/main/nama_penjual%20(single).csv')['name']

  # Combine 2 datasets into 1
  combine = pd.DataFrame({'nama_benar': df_benar, 'nama_campuran': df_salah})

  # Parameter
  vocab_size = 1000
  embedding_dim = 16
  oov_tok = "<OOV>"
  max_sequence_length= 5
  training_portion = .8

  labels = combine['nama_benar'].values.tolist()
  sentences = combine['nama_campuran'].values.tolist()

  training_size = int(len(sentences) * training_portion)

  training_sentences = sentences[:training_size]  # 520 yg benar
  validation_sentences = sentences[training_size:]  # 130 yg benar
  training_labels = labels[:training_size]  # 520 yg benar
  validation_labels = labels[training_size:] # 130 yg benar


  # tokenizing dataset salah
  tokenizer = Tokenizer(num_words=vocab_size,
                        oov_token=oov_tok)
  tokenizer.fit_on_texts(training_sentences)
  word_index = tokenizer.word_index

  # ini baru yg sentence (dataset salah)
  training_sequences = tokenizer.texts_to_sequences(training_sentences)  # sequence 800 yg salah
  training_padded_sequences = pad_sequences(training_sequences,
                                            maxlen=max_sequence_length)  # padded 800 seqeunce yg salah
  validation_sequences = tokenizer.texts_to_sequences(validation_sentences)  # sequence 200 yg salah
  validation_padded_sequences = pad_sequences(validation_sequences,
                                              maxlen=max_sequence_length)  # padded 200 seqeunce yg salah

  # Convert all elements in labels to strings
  #training_labels = [str(labels) for labels in training_labels if isinstance(labels, str)]
  #validation_labels= [str(labels) for labels in validation_labels if isinstance(labels, str)]
  #labels = [str(label) for label in labels]

  # tokenizing dataset benar
  label_tokenizer = Tokenizer(num_words=vocab_size,
                              oov_token=oov_tok)
  label_tokenizer.fit_on_texts(labels)

  # ini baru yg labels (dataset benar)
  training_label_sequences = np.array(label_tokenizer.texts_to_sequences(training_labels))
  validation_label_sequences = np.array(label_tokenizer.texts_to_sequences(validation_labels))

  # Pad the label sequences
  training_label_sequences_padded = pad_sequences(training_label_sequences,
                                                  maxlen=max_sequence_length)
  validation_label_sequences_padded = pad_sequences(validation_label_sequences,
                                                    maxlen=max_sequence_length)

  # model machine learning
  model = tf.keras.Sequential([
      tf.keras.layers.Embedding(input_dim=vocab_size,
                                output_dim=50,
                                input_length=max_sequence_length),
      tf.keras.layers.Dense(len(label_tokenizer.word_index) + 1, activation='relu'),
      tf.keras.layers.Dropout(0.5),
      tf.keras.layers.Dense(len(label_tokenizer.word_index) + 1, activation='softmax')
  ])

  # Use 'sparse_categorical_crossentropy' as the loss function
  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])

  #early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',
  #                                                 patience=10,
    #                                                restore_best_weights=True)

  callbacks=myCallback()

  # Train the model
  history = model.fit(training_padded_sequences,
                      training_label_sequences_padded,
                      epochs=250,
                      batch_size=32,
                      validation_data=(validation_padded_sequences,
                                      validation_label_sequences_padded),
                      #callbacks=[early_stopping],
                      callbacks=callbacks,
                      verbose=2)

  # Correction MANTAP
  # Function for name correction
  def correct_name(text):
      sequence = tokenizer.texts_to_sequences([text])
      padded_sequence = pad_sequences(sequence,
                                      maxlen=len(training_padded_sequences[0]),
                                      padding='pre')

      prediction = model.predict(padded_sequence)
      predicted_sequence = np.argmax(prediction, axis=-1)

      # Extract the last element of the predicted sequence
      predicted_token = predicted_sequence [0][-1]
      print(f"Predicted Token: {predicted_token}")

      # Convert predicted sequence back to text
      corrected_text = label_tokenizer.index_word.get(predicted_token, '')

      return corrected_text

  # Example usage
  input_text = 'fauzan'
  output_text = correct_name(input_text)

  print(f'Search: {input_text}')
  print(f'Did you mean: {output_text}')

  return model

    # The code below is to save your model as a .h5 file.
    # It will be saved automatically in your Submission folder.
if __name__ == '__main__':
    # DO NOT CHANGE THIS CODE
    model = solution_B4()
    model.save("model_B4.h5")

# Load the model
model = load_model('model_B4.h5')

# Convert the model to TFLite format
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
open("model_B4.tflite", "wb").write(tflite_model)

# Save the model in JSON format
model_json = model.to_json()
with open("model_B4.json", "w") as json_file:
    json_file.write(model_json)