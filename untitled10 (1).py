# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r_qQeloR23GkXJltn1yN5t7NGk6tg41t
"""

# install library symspellpy
!pip install symspellpy

from symspellpy import SymSpell, Verbosity

# initialize
sym_spell = SymSpell()

# create dictionary
path_corpus = '/content/nama_penjual (single).csv'
sym_spell.create_dictionary(path_corpus)

# melihat kandidat dari kata 'kcing'
suggestions = sym_spell.lookup('fauzan',
                               Verbosity.CLOSEST,
                               max_edit_distance=2,
                               include_unknown=False)
for s in suggestions:
    print(s.term, s.count)

import json
import tensorflow as tf
from symspellpy import SymSpell, Verbosity

def solution_A4():
    # initialize
    sym_spell = SymSpell()

# create dictionary
    path_corpus = '/content/nama_penjual (single).csv'
    sym_spell.create_dictionary(path_corpus)

    # Split the dataset into two separate lists: full_names and first_names
    first_names = []
    with open('nama_penjual (single).csv', 'r') as file:
        for row in file:
            first_names.append(row.strip())

    test_words = ['fauzan', 'gusti', 'Zah', 'rahman', 'rical',
                  'ern', 'adela', 'kusniah', 'surosu', 'suparti', 'tono', 'andre', 'fikri', 'alex']

    for w in test_words:
        suggestions = sym_spell.lookup(w,
                                      Verbosity.CLOSEST,
                                      max_edit_distance=2,
                                      include_unknown=False)

        print(f'Search: {w}')


        if w in first_names:
            print("_")
        elif suggestions and w != suggestions[0].term:
            print(f'Did you mean: {suggestions[0].term}')
        else:
            print(f"We cannot find {w}")
        print('\n')

    return model


# The code below is to save your model as a .h5 file.
# It will be saved automatically in your Submission folder.
if __name__ == '__main__':
    # DO NOT CHANGE THIS CODE
    model = solution_A4()
    model.save("model_A4.h5")

# initialize
sym_spell = SymSpell()

# create dictionary
path_corpus = '/content/nama_penjual (single).csv'
sym_spell.create_dictionary(path_corpus)

# Split the dataset into two separate lists: full_names and first_names
first_names = []
with open('nama_penjual (single).csv', 'r') as file:
    for row in file:
        first_names.append(row.strip())

test_words = ['fauzan', 'gusti', 'Zah', 'rahman', 'rical',
              'ern', 'adela', 'kusniah', 'surosu', 'suparti', 'tono', 'andre', 'fikri', 'alex']

for w in test_words:
    suggestions = sym_spell.lookup(w,
                                   Verbosity.CLOSEST,
                                   max_edit_distance=2,
                                   include_unknown=False)

    print(f'Search: {w}')


    if w in first_names:
        print("_")
    elif suggestions and w != suggestions[0].term:
        print(f'Did you mean: {suggestions[0].term}')
    else:
        print(f"We cannot find {w}")
    print('\n')

# Save output as JSON
with open('output.json', 'w') as json_file:
    json.dump(output_data, json_file)

# Assuming you have a trained TensorFlow model, convert it to TensorFlow Lite format
# Replace 'your_model_path' with the actual path to your trained model
your_model_path = 'path/to/your/trained_model.h5'

# Load the model, unpacking the tuple if necessary
loaded_model = tf.keras.models.load_model(your_model_path)
if isinstance(loaded_model, tuple):
    model = loaded_model[0]
else:
    model = loaded_model

# Save the Keras model in .h5 format
model.save('model.h5')

# Convert the model to TFLite format
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the TFLite model to a file
with open('model.tflite', 'wb') as tflite_file:
    tflite_file.write(tflite_model)

# if __name__ == '_main_':
    # DO NOT CHANGE THIS CODE
# model = ()
# model.save("model_A1.h5")


# # with open('output.json', 'w') as json_file:
# #   json.dump(output_data, json_file)

# # your_model_path = 'path/to/your/trained_model.h5'

# # model = tf.keras.models.load_model(your_model_path)

# # Convert the model to TFLite format
# converter = tf.lite.TFLiteConverter.from_keras_model(model)
# tflite_model = converter.convert()

# # Save the TFLite model to a file
# with open('model.tflite', 'wb') as tflite_file:
#     tflite_file.write(tflite_model)

import json
import tensorflow as tf
from symspellpy import SymSpell, Verbosity

# initialize
sym_spell = SymSpell()

# create dictionary
path_corpus = '/content/nama_penjual (single).csv'
sym_spell.create_dictionary(path_corpus)

# Split the dataset into two separate lists: full_names and first_names
first_names = []
with open('nama_penjual (single).csv', 'r') as file:
    for row in file:
        first_names.append(row.strip())

test_words = ['fauzan', 'gusti', 'Zah', 'rahman', 'rical',
              'ern', 'adela', 'kusniah', 'surosu', 'suparti', 'tono', 'andre', 'fikri', 'alex']

output_data = []

for w in test_words:
    suggestions = sym_spell.lookup(w,
                                   Verbosity.CLOSEST,
                                   max_edit_distance=2,
                                   include_unknown=False)

    result = {'search': w}

    if w in first_names:
        result['output'] = "_"
    elif suggestions and w != suggestions[0].term:
        result['output'] = f'Did you mean: {suggestions[0].term}'
    else:
        result['output'] = f"We cannot find {w}"

    output_data.append(result)

    print(result)
    print('\n')


# Save output as JSON
with open('output.json', 'w') as json_file:
    json.dump(output_data, json_file)

# Assuming you have a trained TensorFlow model, convert it to TensorFlow Lite format
# Replace 'your_model_path' with the actual path to your trained model
your_model_path = 'path/to/your/trained_model.h5'

# Load the model, unpacking the tuple if necessary
loaded_model = tf.keras.models.load_model(your_model_path)
if isinstance(loaded_model, tuple):
    model = loaded_model[0]
else:
    model = loaded_model

# Save the Keras model in .h5 format
model.save('model.h5')

# Convert the model to TFLite format
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the TFLite model to a file
with open('model.tflite', 'wb') as tflite_file:
    tflite_file.write(tflite_model)

!pip install symspellpy
import json
import tensorflow as tf
from symspellpy import SymSpell, Verbosity

def solution_A4():
  sym_spell = SymSpell()

  path_corpus = '/content/nama_penjual (single).csv'
  sym_spell.create_dictionary(path_corpus)

  first_names = []
  with open('/content/nama_penjual (single).csv', 'r') as file:
    for row in file:
      first_names.append(row.strip())

  test_words = []

  # test_words = ['fauzan', 'gusti', 'Zah', 'rahman', 'rical',
  #                 'ern', 'adela', 'kusniah', 'surosu', 'suparti', 'tono', 'andre', 'fikri', 'alex']

  output_data = []

  for w in test_words:
    suggestions = sym_spell.lookup(w,
                                   Verbosity.CLOSEST,
                                   max_edit_distance=2,
                                   include_unknown=False)
    result = {'search': w}

    if w in first_names:
        result['output'] = " "
    elif suggestions and w != suggestions[0].term:
      result['output'] = f'Did you mean: {suggestions[0].term}'
    else:
        result['output'] = f"We cannot find {w}"

  return result


# test_words = ['fauzan', 'gusti', 'Zah', 'rahman', 'rical',
#                   'ern', 'adela', 'kusniah', 'surosu', 'suparti', 'tono', 'andre', 'fikri', 'alex']
# for w in test_words:
#   suggestions = sym_spell.lookup(w,
#                                    Verbosity.CLOSEST,
#                                    max_edit_distance=2,
#                                    include_unknown=False)

# print(result)
# The code below is to save your model as a .h5 file.
# It will be saved automatically in your Submission folder.
if __name__ == '__main__':
    # DO NOT CHANGE THIS CODE
    model = solution_A4()
    model.save("model_A4.h5")

import json
import tensorflow as tf
from symspellpy import SymSpell, Verbosity

# initialize
sym_spell = SymSpell()

path_corpus = '/content/nama_penjual (single).csv'
sym_spell.create_dictionary(path_corpus)

# Define the function to perform the lookup
def perform_lookup(word):
    suggestions = sym_spell.lookup(word,
                                   Verbosity.CLOSEST,
                                   max_edit_distance=2,
                                   include_unknown=False)

    if word in first_names:
        return " "
    elif suggestions and word != suggestions[0].term:
        return f'Did you mean: {suggestions[0].term}'
    else:
        return f"We cannot find {word}"

# Convert the function to a TensorFlow function
converter = tf.lite.TFLiteConverter.from_concrete_functions([perform_lookup.get_concrete_function(tf.TensorSpec(shape=None, dtype=tf.string))])

# Convert to TensorFlow Lite model
tflite_model = converter.convert()

# Save the model to a file
with open('sym_spell_model.tflite', 'wb') as f:
    f.write(tflite_model)

import tensorflow as tf
from symspellpy import SymSpell, Verbosity

# initialize
sym_spell = SymSpell()

# create dictionary
path_corpus = '/content/nama_penjual (single).csv'
sym_spell.create_dictionary(path_corpus)

# Define the function to perform the lookup
@tf.function
def perform_lookup(word):
    suggestions = sym_spell.lookup(word,
                                   Verbosity.CLOSEST,
                                   max_edit_distance=2,
                                   include_unknown=False)

    if word in first_names:
        return " "
    elif suggestions and suggestions[0].term != word:
        return f'Did you mean: {suggestions[0].term}'
    else:
        return f"We cannot find {word}"

# Convert the function to a TensorFlow Lite model
converter = tf.lite.TFLiteConverter.from_concrete_functions([perform_lookup.get_concrete_function(tf.TensorSpec(shape=None, dtype=tf.string))])

# Convert to TensorFlow Lite model
tflite_model = converter.convert()

# Save the model to a file
with open('sym_spell_model.tflite', 'wb') as f:
    f.write(tflite_model)

# versi 2 yg JAGOAN
# Train dataset kecil MANTAP

import tensorflow as tf
import pandas as pd
import numpy as np
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if (logs.get('val_accuracy')>0.8353):
          self.model.stop_training = True

def solution_B4():
  # Load dataset yg super duper banyak
  df_benar = pd.read_csv('https://raw.githubusercontent.com/Zahraa02/Roadgo/main/nama_penjual%20(single).csv')['name'] # cb pake lgsg docs yg diupload # cb pake dataset yg kecil
  df_salah = pd.read_csv('https://raw.githubusercontent.com/Zahraa02/Roadgo/main/nama_penjual%20(single).csv')['name']

  # Combine 2 datasets into 1
  combine = pd.DataFrame({'nama_benar': df_benar, 'nama_campuran': df_salah})

  # Parameter
  vocab_size = 1000
  embedding_dim = 16
  oov_tok = "<OOV>"
  max_sequence_length= 5
  training_portion = .8

  labels = combine['nama_benar'].values.tolist()
  sentences = combine['nama_campuran'].values.tolist()

  training_size = int(len(sentences) * training_portion)

  training_sentences = sentences[:training_size]  # 520 yg benar
  validation_sentences = sentences[training_size:]  # 130 yg benar
  training_labels = labels[:training_size]  # 520 yg benar
  validation_labels = labels[training_size:] # 130 yg benar


  # tokenizing dataset salah
  tokenizer = Tokenizer(num_words=vocab_size,
                        oov_token=oov_tok)
  tokenizer.fit_on_texts(training_sentences)
  word_index = tokenizer.word_index

  # ini baru yg sentence (dataset salah)
  training_sequences = tokenizer.texts_to_sequences(training_sentences)  # sequence 800 yg salah
  training_padded_sequences = pad_sequences(training_sequences,
                                            maxlen=max_sequence_length)  # padded 800 seqeunce yg salah
  validation_sequences = tokenizer.texts_to_sequences(validation_sentences)  # sequence 200 yg salah
  validation_padded_sequences = pad_sequences(validation_sequences,
                                              maxlen=max_sequence_length)  # padded 200 seqeunce yg salah

  # Convert all elements in labels to strings
  #training_labels = [str(labels) for labels in training_labels if isinstance(labels, str)]
  #validation_labels= [str(labels) for labels in validation_labels if isinstance(labels, str)]
  #labels = [str(label) for label in labels]

  # tokenizing dataset benar
  label_tokenizer = Tokenizer(num_words=vocab_size,
                              oov_token=oov_tok)
  label_tokenizer.fit_on_texts(labels)

  # ini baru yg labels (dataset benar)
  training_label_sequences = np.array(label_tokenizer.texts_to_sequences(training_labels))
  validation_label_sequences = np.array(label_tokenizer.texts_to_sequences(validation_labels))

  # Pad the label sequences
  training_label_sequences_padded = pad_sequences(training_label_sequences,
                                                  maxlen=max_sequence_length)
  validation_label_sequences_padded = pad_sequences(validation_label_sequences,
                                                    maxlen=max_sequence_length)

  # model machine learning
  model = tf.keras.Sequential([
      tf.keras.layers.Embedding(input_dim=vocab_size,
                                output_dim=50,
                                input_length=max_sequence_length),
      tf.keras.layers.Dense(len(label_tokenizer.word_index) + 1, activation='relu'),
      tf.keras.layers.Dropout(0.5),
      tf.keras.layers.Dense(len(label_tokenizer.word_index) + 1, activation='softmax')
  ])

  # Use 'sparse_categorical_crossentropy' as the loss function
  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])

  #early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',
  #                                                 patience=10,
    #                                                restore_best_weights=True)

  callbacks=myCallback()

  # Train the model
  history = model.fit(training_padded_sequences,
                      training_label_sequences_padded,
                      epochs=250,
                      batch_size=32,
                      validation_data=(validation_padded_sequences,
                                      validation_label_sequences_padded),
                      #callbacks=[early_stopping],
                      callbacks=callbacks,
                      verbose=2)

  # Correction MANTAP
  # Function for name correction
  def correct_name(text):
      sequence = tokenizer.texts_to_sequences([text])
      padded_sequence = pad_sequences(sequence,
                                      maxlen=len(training_padded_sequences[0]),
                                      padding='pre')

      prediction = model.predict(padded_sequence)
      predicted_sequence = np.argmax(prediction, axis=-1)

      # Extract the last element of the predicted sequence
      predicted_token = predicted_sequence [0][-1]
      print(f"Predicted Token: {predicted_token}")

      # Convert predicted sequence back to text
      corrected_text = label_tokenizer.index_word.get(predicted_token, '')

      return corrected_text

  # Example usage
  input_text = 'fauzan'
  output_text = correct_name(input_text)

  print(f'Search: {input_text}')
  print(f'Did you mean: {output_text}')

  return model

    # The code below is to save your model as a .h5 file.
    # It will be saved automatically in your Submission folder.
if __name__ == '__main__':
    # DO NOT CHANGE THIS CODE
    model = solution_B4()
    model.save("model_B4.h5")

# # versi 2 yg JAGOAN
# Train dataset kecil MANTAP

import tensorflow as tf
import pandas as pd
import numpy as np
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if (logs.get('val_accuracy')>0.8353):
          self.model.stop_training = True

def solution_B4():
  # Load dataset yg super duper banyak
  df_benar = pd.read_csv('https://raw.githubusercontent.com/Zahraa02/Roadgo/main/nama_penjual%20(single).csv')['name'] # cb pake lgsg docs yg diupload # cb pake dataset yg kecil
  df_salah = pd.read_csv('https://raw.githubusercontent.com/Zahraa02/Roadgo/main/nama_penjual%20(single).csv')['name']

  # Combine 2 datasets into 1
  combine = pd.DataFrame({'nama_benar': df_benar, 'nama_campuran': df_salah})

  # Parameter
  vocab_size = 1000
  embedding_dim = 16
  oov_tok = "<OOV>"
  max_sequence_length= 5
  training_portion = .8

  labels = combine['nama_benar'].values.tolist()
  sentences = combine['nama_campuran'].values.tolist()

  training_size = int(len(sentences) * training_portion)

  training_sentences = sentences[:training_size]  # 520 yg benar
  validation_sentences = sentences[training_size:]  # 130 yg benar
  training_labels = labels[:training_size]  # 520 yg benar
  validation_labels = labels[training_size:] # 130 yg benar


  # tokenizing dataset salah
  tokenizer = Tokenizer(num_words=vocab_size,
                        oov_token=oov_tok)
  tokenizer.fit_on_texts(training_sentences)
  word_index = tokenizer.word_index

  # ini baru yg sentence (dataset salah)
  training_sequences = tokenizer.texts_to_sequences(training_sentences)  # sequence 800 yg salah
  training_padded_sequences = pad_sequences(training_sequences,
                                            maxlen=max_sequence_length)  # padded 800 seqeunce yg salah
  validation_sequences = tokenizer.texts_to_sequences(validation_sentences)  # sequence 200 yg salah
  validation_padded_sequences = pad_sequences(validation_sequences,
                                              maxlen=max_sequence_length)  # padded 200 seqeunce yg salah

  # Convert all elements in labels to strings
  #training_labels = [str(labels) for labels in training_labels if isinstance(labels, str)]
  #validation_labels= [str(labels) for labels in validation_labels if isinstance(labels, str)]
  #labels = [str(label) for label in labels]

  # tokenizing dataset benar
  label_tokenizer = Tokenizer(num_words=vocab_size,
                              oov_token=oov_tok)
  label_tokenizer.fit_on_texts(labels)

  # ini baru yg labels (dataset benar)
  training_label_sequences = np.array(label_tokenizer.texts_to_sequences(training_labels))
  validation_label_sequences = np.array(label_tokenizer.texts_to_sequences(validation_labels))

  # Pad the label sequences
  training_label_sequences_padded = pad_sequences(training_label_sequences,
                                                  maxlen=max_sequence_length)
  validation_label_sequences_padded = pad_sequences(validation_label_sequences,
                                                    maxlen=max_sequence_length)

  # model machine learning
  model = tf.keras.Sequential([
      tf.keras.layers.Embedding(input_dim=vocab_size,
                                output_dim=50,
                                input_length=max_sequence_length),
      tf.keras.layers.Dense(len(label_tokenizer.word_index) + 1, activation='relu'),
      tf.keras.layers.Dropout(0.5),
      tf.keras.layers.Dense(len(label_tokenizer.word_index) + 1, activation='softmax')
  ])

  # Use 'sparse_categorical_crossentropy' as the loss function
  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])

  #early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',
  #                                                 patience=10,
    #                                                restore_best_weights=True)

  callbacks=myCallback()

  # Train the model
  history = model.fit(training_padded_sequences,
                      training_label_sequences_padded,
                      epochs=250,
                      batch_size=32,
                      validation_data=(validation_padded_sequences,
                                      validation_label_sequences_padded),
                      #callbacks=[early_stopping],
                      callbacks=callbacks,
                      verbose=2)

  # Correction MANTAP
  # Function for name correction
  def correct_name(text):
      sequence = tokenizer.texts_to_sequences([text])
      padded_sequence = pad_sequences(sequence,
                                      maxlen=len(training_padded_sequences[0]),
                                      padding='pre')

      prediction = model.predict(padded_sequence)
      predicted_sequence = np.argmax(prediction, axis=-1)

      # Extract the last element of the predicted sequence
      predicted_token = predicted_sequence [0][-1]
      print(f"Predicted Token: {predicted_token}")

      # Convert predicted sequence back to text
      corrected_text = label_tokenizer.index_word.get(predicted_token, '')

      return corrected_text

  # Example usage
  input_text = 'fauzar'
  output_text = correct_name(input_text)

  print(f'Search: {input_text}')
  print(f'Did you mean: {output_text}')

  return model

  export_dir='saved_model/1'

#SUDAH .json .tflite .h5
# versi 2 yg JAGOAN
# Train dataset kecil MANTAP

import tensorflow as tf
import pandas as pd
import numpy as np
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if (logs.get('val_accuracy')>0.8353):
          self.model.stop_training = True

def solution_B4():
  # Load dataset yg super duper banyak
  df_benar = pd.read_csv('https://raw.githubusercontent.com/Zahraa02/Roadgo/main/nama_penjual%20(single).csv')['name'] # cb pake lgsg docs yg diupload # cb pake dataset yg kecil
  df_salah = pd.read_csv('https://raw.githubusercontent.com/Zahraa02/Roadgo/main/nama_penjual%20(single).csv')['name']

  # Combine 2 datasets into 1
  combine = pd.DataFrame({'nama_benar': df_benar, 'nama_campuran': df_salah})

  # Parameter
  vocab_size = 1000
  embedding_dim = 16
  oov_tok = "<OOV>"
  max_sequence_length= 5
  training_portion = .8

  labels = combine['nama_benar'].values.tolist()
  sentences = combine['nama_campuran'].values.tolist()

  training_size = int(len(sentences) * training_portion)

  training_sentences = sentences[:training_size]  # 520 yg benar
  validation_sentences = sentences[training_size:]  # 130 yg benar
  training_labels = labels[:training_size]  # 520 yg benar
  validation_labels = labels[training_size:] # 130 yg benar


  # tokenizing dataset salah
  tokenizer = Tokenizer(num_words=vocab_size,
                        oov_token=oov_tok)
  tokenizer.fit_on_texts(training_sentences)
  word_index = tokenizer.word_index

  # ini baru yg sentence (dataset salah)
  training_sequences = tokenizer.texts_to_sequences(training_sentences)  # sequence 800 yg salah
  training_padded_sequences = pad_sequences(training_sequences,
                                            maxlen=max_sequence_length)  # padded 800 seqeunce yg salah
  validation_sequences = tokenizer.texts_to_sequences(validation_sentences)  # sequence 200 yg salah
  validation_padded_sequences = pad_sequences(validation_sequences,
                                              maxlen=max_sequence_length)  # padded 200 seqeunce yg salah

  # Convert all elements in labels to strings
  #training_labels = [str(labels) for labels in training_labels if isinstance(labels, str)]
  #validation_labels= [str(labels) for labels in validation_labels if isinstance(labels, str)]
  #labels = [str(label) for label in labels]

  # tokenizing dataset benar
  label_tokenizer = Tokenizer(num_words=vocab_size,
                              oov_token=oov_tok)
  label_tokenizer.fit_on_texts(labels)

  # ini baru yg labels (dataset benar)
  training_label_sequences = np.array(label_tokenizer.texts_to_sequences(training_labels))
  validation_label_sequences = np.array(label_tokenizer.texts_to_sequences(validation_labels))

  # Pad the label sequences
  training_label_sequences_padded = pad_sequences(training_label_sequences,
                                                  maxlen=max_sequence_length)
  validation_label_sequences_padded = pad_sequences(validation_label_sequences,
                                                    maxlen=max_sequence_length)

  # model machine learning
  model = tf.keras.Sequential([
      tf.keras.layers.Embedding(input_dim=vocab_size,
                                output_dim=50,
                                input_length=max_sequence_length),
      tf.keras.layers.Dense(len(label_tokenizer.word_index) + 1, activation='relu'),
      tf.keras.layers.Dropout(0.5),
      tf.keras.layers.Dense(len(label_tokenizer.word_index) + 1, activation='softmax')
  ])

  # Use 'sparse_categorical_crossentropy' as the loss function
  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])

  #early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',
  #                                                 patience=10,
    #                                                restore_best_weights=True)

  callbacks=myCallback()

  # Train the model
  history = model.fit(training_padded_sequences,
                      training_label_sequences_padded,
                      epochs=250,
                      batch_size=32,
                      validation_data=(validation_padded_sequences,
                                      validation_label_sequences_padded),
                      #callbacks=[early_stopping],
                      callbacks=callbacks,
                      verbose=2)

  # Correction MANTAP
  # Function for name correction
  def correct_name(text):
      sequence = tokenizer.texts_to_sequences([text])
      padded_sequence = pad_sequences(sequence,
                                      maxlen=len(training_padded_sequences[0]),
                                      padding='pre')

      prediction = model.predict(padded_sequence)
      predicted_sequence = np.argmax(prediction, axis=-1)

      # Extract the last element of the predicted sequence
      predicted_token = predicted_sequence [0][-1]
      print(f"Predicted Token: {predicted_token}")

      # Convert predicted sequence back to text
      corrected_text = label_tokenizer.index_word.get(predicted_token, '')

      return corrected_text

  # Example usage
  input_text = 'fauzan'
  output_text = correct_name(input_text)

  print(f'Search: {input_text}')
  print(f'Did you mean: {output_text}')

  return model

    # The code below is to save your model as a .h5 file.
    # It will be saved automatically in your Submission folder.
if __name__ == '__main__':
    # DO NOT CHANGE THIS CODE
    model = solution_B4()
    model.save("model_B4.h5")

# Load the model
model = load_model('model_B4.h5')

# Convert the model to TFLite format
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
open("model_B4.tflite", "wb").write(tflite_model)

# Save the model in JSON format
model_json = model.to_json()
with open("model_B4.json", "w") as json_file:
    json_file.write(model_json)